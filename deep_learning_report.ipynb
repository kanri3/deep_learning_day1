{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_report.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFD+UezUkUeWJZkpPak6Kp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanri3/deep_learning_day1_day2/blob/main/deep_learning_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv3wLRJk_vjZ"
      },
      "source": [
        "# 深層学習day1 レポート"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "825Sq5aU_tBH"
      },
      "source": [
        "## 1.1 入力層～中間層"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgE_45oYZ4qY"
      },
      "source": [
        "### NN（ニューラルネットワーク）の各層  \n",
        "入力層，隠れ層（もしくは中間層），出力層  \n",
        "\n",
        "各層は複数の「ノード（もしくはユニット）」が「エッジ」で結ばれる構造  \n",
        "隠れ層を多数持つもの：深層学習（ディープラーニング）  \n",
        "\n",
        "各層は「活性化関数」と呼ばれる関数を持ち、エッジは「重み」を持つ。  \n",
        "### 各ノードの値を決めるもの  \n",
        "前の層のノードの値，接続エッジの重みの値，層が持つ活性化関数から計算される。  \n",
        "\n",
        "### 提言  \n",
        "後で重みは逆伝播で修正されることを最初によく伝えておかないと、混乱を招く。  \n",
        "実装演習で、乱数で無意味に決めた重み行列で分類ができるみたいなことをやってるから、  \n",
        "「これで分類できるわけないのに…、一体何をやらされてるんだろう」となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz0IzBsRdMY9"
      },
      "source": [
        "## 1.2 活性化関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZnB60e-vuX7"
      },
      "source": [
        "### Relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHa8yvIjvzsL"
      },
      "source": [
        "グラフの形状から「ランプ関数」（ramp function）とも  \n",
        "ランプ ramp：「傾斜路」のこと。高速道路に入るための上り坂の様な  \n",
        "（電灯のランプではないのだ！）  \n",
        "座標点(0, 0)を基点とした傾斜路か…  \n",
        "\n",
        "https://www.atmarkit.co.jp/ait/articles/2003/11/news016.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RihJCmsbWq1i"
      },
      "source": [
        "### 誤差関数（損失関数）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn3-RFFLEiJu"
      },
      "source": [
        "予想データと正解データの出力の間にどのくらい誤差があるのかを評価する関数。  \n",
        "作成した予測モデルの精度を評価する際に使われ、値が小さいほど正確なモデルと言える。  \n",
        "https://ai-kenkyujo.com/term/loss-function-error-function/  \n",
        "\n",
        "### 提言  \n",
        "確認テストで誤差関数（二乗誤差）に$\\frac{1}{2}$掛けてるのは何故か？とわざわざ問うているから、  \n",
        "よほど大事なことを言うのかと期待したら、  \n",
        "「本質的な意味はない」って…、正直残念な問いだね。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhTm1150W1HZ"
      },
      "source": [
        "### 交差エントロピー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94KegEknFS8i"
      },
      "source": [
        "分類問題における予測の誤差として使われることが多い。  \n",
        "$P$ を「真の分布」、$Q$ を「予測の分布」とすると、予測が真に似ているほど、$P$ と $Q$ の交差エントロピーは小さくなる。  \n",
        "https://mathwords.net/kousaentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH7FQyxmd6XP"
      },
      "source": [
        "## 1.3 出力"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xiqz15hWZAB"
      },
      "source": [
        "### ソフトマックス関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSuStbR4AR-Z"
      },
      "source": [
        "Softmax function もしくは正規化指数関数： Normalized exponential function  \n",
        "複数の出力値の合計が1.0（＝100％）になるように変換して出力する関数である。各出力値の範囲は0.0～1.0となる。  \n",
        "ソフト：滑らかな曲線  \n",
        "マックス：1つの出力値が最大となる  \n",
        "分類問題における出力層の活性化関数として用いられる。（その場合、損失関数には交差エントロピー（Cross entropy）が用いられる）  \n",
        "https://www.atmarkit.co.jp/ait/articles/2004/08/news016.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3XR66RmW360"
      },
      "source": [
        "### 表1　問題タイプ毎の活性化関数・損失関数の組み合わせ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au-rJPf3L3kQ"
      },
      "source": [
        "|＼|分類問題（二値）|分類問題（多クラス）|回帰問題|\n",
        "|:---------------|:----------------|:----------------|:----------------|\n",
        "|出力層の活性化関数|シグモイド関数|ソフトマックス関数|（なし）／恒等関数|\n",
        "|対応する損失関数|二値用の交差エントロピー<br>（Binary cross entropy）誤差|多クラス用の交差エントロピー<br>（Categorical cross entropy）誤差|平均二乗誤差（MSE）|\n",
        "\n",
        "https://www.atmarkit.co.jp/ait/articles/2003/26/news012.html  \n",
        "恒等関数：  \n",
        "https://www.atmarkit.co.jp/ait/articles/2004/01/news045.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiEnntFWIMyW"
      },
      "source": [
        "## 1.4 勾配降下法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOp0p9RuxQsu"
      },
      "source": [
        "|手法|利用データ|計算時間|特徴|\n",
        "|:-------|:--------|:--------:|:--------|\n",
        "|バッチ勾配降下法<br>(最急降下法)|全てのデータを利用|大|局所最適解に陥りやすい|\n",
        "|SGD<br>(確率的勾配降下法)|1つのデータを利用|小|外れ値の影響を大きく受ける|\n",
        "|ミニバッチ勾配降下法|一部のデータを利用|中|SGDとバッチ勾配降下法両方の特徴<br>（バランスが良い？）|\n",
        "\n",
        "バッチ勾配降下法の局所最適解  \n",
        "https://axa.biopapyrus.jp/deep-learning/gradient_descent_method.html  \n",
        "\n",
        "### 以下、確認テストより  \n",
        "☆オンライン学習とは  \n",
        "個々の訓練データが投入される都度パラメータを更新するやり方。ここでは  \n",
        "$$オンライン学習＝SGD$$\n",
        "\n",
        "☆数式の意味  \n",
        "$$w^{(t+1)}=w^{(t)}-{\\epsilon}{\\nabla}E_t$$  \n",
        "損失関数の勾配が負 → 重みが増える  \n",
        "損失関数の勾配が$0$になるまで重みは増え続けることに。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar0xn39SdqzN"
      },
      "source": [
        "## 1.5 誤差逆伝播法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_fjqCeqKXHW"
      },
      "source": [
        "算出された誤差を出力層側から順に偏微分し、その前の層、さらに前の層と伝播していく。\n",
        "\n",
        "back propagation  \n",
        "propagate（伝播する）  \n",
        "propaganda（政治的な宣伝活動）と語源が同じ  \n",
        "\n",
        "解析的に解く（今回がそれ）…数式の変形で答えを得る。  \n",
        "⇔ 数値的に解く  \n",
        "\n",
        "確認テストより  \n",
        "「誤差逆伝播法は、不要な再帰的処理を避けることができる。」  \n",
        "要は、先に行なった偏微分の結果をさらに偏微分していくことになるから、  \n",
        "先に行なった偏微分の結果を後で使えば効率的だって言ってるんだね。  \n",
        "\n",
        "「不要な再帰処理」＝「同じ計算を繰り返しすること」  \n",
        "不要な難しい言い方だね"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhQeaEjgKmdX"
      },
      "source": [
        "# 深層学習day2 レポート"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A_2EZjv8Jl-"
      },
      "source": [
        "## 2.1 勾配消失問題  \n",
        "以下のcolabノートで実装演習した。  \n",
        "2_1_1_network_modified_hands_on.ipynb  \n",
        "2_1_2_vanishing_gradient_hands_on.ipynb  \n",
        "2_1_3_batch_normalization_hands_on.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRMQyzNW8QCr"
      },
      "source": [
        "勾配消失問題の原因のひとつ → 活性化関数  \n",
        "シグモイド関数には、勾配が0に近い領域が存在  \n",
        "→ 勾配消失 → 重みがほぼ修正されなくなる。  \n",
        "\n",
        "多層ニューラルネットワークでは一カ所でも勾配が0に近い層が存在すると、それより下層の勾配も全て0に近くなる。\n",
        "\n",
        "活性化関数の変更  \n",
        "シグモイド関数の代わりにランプ関数（ReLUなど）を用いる  \n",
        "→ 勾配消失問題を改善"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0QBYqhGvMvq"
      },
      "source": [
        "### 重みの初期化の課題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGQnJwKOvLHG"
      },
      "source": [
        "https://ntacoffee.com/xavier-initialization/#i-2  \n",
        "おさらい  \n",
        "ニューラルネットワークとは、入力と出力の間の非線形な関係をパラメータ（重み）を使ってモデル化して、そのパラメータを誤差逆伝搬によって最適化していくもの  \n",
        "\n",
        "パラメータの初期値次第では、学習に非常に長い時間がかかってしまう。  \n",
        "（最初の出力が理想的な値から大きく外れていたり、勾配が非常に大きい・小さい値をとってしまったり）  \n",
        "\n",
        "パラメータの初期値として用いられるのは、  \n",
        "一様分布や正規分布からサンプリングされた値  \n",
        "これらはうまくスケーリングをしてあげないと…。  \n",
        "しれっと、スケーリング？  \n",
        "https://newtechnologylifestyle.net/1363-2/  \n",
        "近年、スケーリングを経験的ではなく理論的に決める二つの手法「Xavierの初期化」と「Kaimingの初期化」（Heのこと）が提案された。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHE5ycI8pPDx"
      },
      "source": [
        "### Xavier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWsU4WR-pWUG"
      },
      "source": [
        "ザビエル（シャビエル）だって  \n",
        "論文著者っぽい（宣教師ではない）  \n",
        "前層から渡されるノード数が$n$個である時、重み行列$W$の各要素を$\\sqrt{n}$で割ってやる。  \n",
        "！！！注！！！活性化関数との相性  \n",
        "Reluとはわるい  \n",
        "https://ntacoffee.com/xavier-initialization/#Xavier-2  \n",
        "「Xavierの初期化の限界」の項"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIKa4Ncr12Y9"
      },
      "source": [
        "### He"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9QTkKQO15kT"
      },
      "source": [
        "前層から渡されるノード数が$n$個である時、重み行列$W$の各要素を$\\sqrt{\\frac{n}{2}}$で割ってやる。  \n",
        "\n",
        "https://ntacoffee.com/kaiming-initialization/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um0ZRqVy-KbH"
      },
      "source": [
        "### バッチ正規化（Batch Normalization）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zf5ju6S-O19"
      },
      "source": [
        "層が深くなるとうまく学習ができなくなる。  \n",
        "→ その時の対処方法としてバッチノーマライゼーション。  \n",
        "処理内容：1ミニバッチ内の全データの同一チャネルを平均$0$,分散$1$に正規化する。  \n",
        "\n",
        "ミニバッチ … 1回に処理する複数画像のカタマリ。  \n",
        "正規化 … データから全体の平均値を引いて、全体の標準偏差で割り算すること。  \n",
        "チャネル … CNNの章を参照  \n",
        "\n",
        "！！注意！！  \n",
        "「正規化する」≠「正規分布にする」  \n",
        "正規化というのは元のデータが従う分布を変えるわけではない。  \n",
        "\n",
        "効用：安定した学習（勾配の爆発 or 消失の回避）  \n",
        "\n",
        "バッチ正規化後、平均$0$分散$1$のデータたちに対して$\\gamma$をかけて$\\beta$を足している。  \n",
        "ミニバッチのデータ(の各チャネル)は平均$\\beta$、分散$\\gamma$を新しく持つことに。  \n",
        "（$\\beta$と$\\gamma$は学習により得られる）  \n",
        "https://yaakublog.com/batch-normalization  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VegTzYZjIJj6"
      },
      "source": [
        "## 2.2 最適化  \n",
        "colabノート『2_2_optimizer_hands_on.ipynb』にて実装演習した。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjAqLhwEIM8B"
      },
      "source": [
        "最適化アルゴリズム(optimizer)とは？  \n",
        "損失関数が最小となる時のパラメータを求めるアルゴリズム  \n",
        "\n",
        "|optimizer|特徴|デメリット|\n",
        "|:---|:---|:---|\n",
        "|SGD$*_1$|シンプル|緩やかな勾配にさしかかった途端に学習が遅くなる|\n",
        "|momentum|SGD＋「速度」と「慣性」の概念|学習率は重みにかかわらず一定|\n",
        "|AdaGrad|それぞれの重みに適応させた学習率になるように<br>勾配降下法の更新式を変更|学習が進むにつれ更新量がどんどん小さくなっていき、<br>場合によっては学習が全くされない|\n",
        "|RMSprop|過去の情報を「忘れる」というコンセプト<br>AdaGrad＋最近の勾配ほど強く影響|-|\n",
        "|Adam|momentumSGD＋RMSprop<br>いいとこどり目指した感じ？|ハイパーパラメータが多いかも|\n",
        "\n",
        "$*_1$このテーマで扱う際は、バッチ勾配降下法，ミニバッチ勾配降下法，確率的勾配降下法を一緒くたにSGDとして扱うことも  \n",
        "https://tech-lab.sios.jp/archives/21823  \n",
        "https://watlab-blog.com/2020/03/09/rmsprop/  \n",
        "\n",
        "配布されたcolabノートのコードには冗長部分（同じ処理の繰り返し）が多く、  \n",
        "また訓練の進行を表示する間隔が小さすぎて、  \n",
        "そのままだと各最適化法の比較がしづらかったので、コードの冗長部分をなくし表示間隔を大きくした所、  \n",
        "添付のcolabノート『2_2_optimizer_hands_on.ipynb』の様に比較しやすくなった。  \n",
        "\n",
        "その結果、一番結果が良かったのがRMSpropとAdamであり、今回の実装演習の範囲では甲乙付けがたい。  \n",
        "事前に決めるパラメータ$^*$が少ない分、RMSpropに軍配か  \n",
        "$*$RMSpropが1個，Adamが2個"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DigFxAL66JBE"
      },
      "source": [
        "## 2.3 過学習 over fitting  \n",
        "以下のcolabノートにて実装演習した。  \n",
        "2_3_1_overfiting_hands_on.ipynb  \n",
        "2_3_2_dropout_hands_on.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeQXJ8NpMBUj"
      },
      "source": [
        "### 正則化 regularization  \n",
        "正則化強度はハイパーパラメータ    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbcP4daf6Mwp"
      },
      "source": [
        "#### L1正則化\n",
        "ラッソ Lasso  \n",
        "複数の重みの中から必要のない重みを無効化する。  \n",
        "スパース(sparse)とは  \n",
        "「まばらな」「希薄な」 という意味だが、データ分析の文脈では\n",
        "\n",
        "「データの本質を表すような情報は、データ中に僅かしか含まれていない」  \n",
        "の意味で使われる。  \n",
        "https://thinkit.co.jp/article/17713  \n",
        "\n",
        "それが更にここでは  \n",
        "スパース性：重みの多くを0と推定する性質  \n",
        "\n",
        "なぜLassoはスパース性をもつのか？（直感的説明）  \n",
        "深層学習day2講義資料（PDF）にある有名な図  \n",
        "「等高線の内側」かつ「正方形の内部」にある点が解に  \n",
        "この時、正方形の角が解になり易い。  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6rDkciq6RRF"
      },
      "source": [
        "#### L2正則化\n",
        "weight decay 荷重減衰  \n",
        "リッジ Ridge  \n",
        "重みの値を調整することでモデルを改善する。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfQzZFrj6TJu"
      },
      "source": [
        "### Dropout  \n",
        "AlexNet（今に至るdeep learningブームの火蓋を切った）が有用性を示したのがDropout  \n",
        "\n",
        "#### 御社提供コードから、Dropoutの実装を見てみる\n",
        "裏にコソコソあるファイルに書かれてる。ここまで見てやる奴少ないんとちゃう？  \n",
        "\n",
        "実装演習『2_5_dropout.ipynb』にて実施  \n",
        "multi_layer_net.pyファイルのクラスMultiLayerNetのコンストラクタの引数に\n",
        "『Dropout率』がある。  \n",
        "更にlayers.pyファイルのクラスDropoutのコンストラクタの引数に\n",
        "Dropout率を渡している。  \n",
        "入力行列の全要素からDropout率だけランダムに要素の値を$0$にする。  \n",
        "（入力行列の要素の値が全て$0～1$の範囲なら$^*$、Dropout率以下の要素を変換対象にする。  \n",
        "\n",
        "$*$手前でBatch normalizationしておけば良い。）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYzEOIQbjQu4"
      },
      "source": [
        "## 2.4 CNNの概念  \n",
        "以下のcolabノートで実装演習した。  \n",
        "2_4_1_simple_convolution_hands_on.ipynb  \n",
        "2_4_2_double_comvolution_hands_on.ipynb  \n",
        "2_4_3_deep_convolution_hands_on.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KegSuW1DjViZ"
      },
      "source": [
        "### 人間の視覚をモデルに考案  \n",
        "\n",
        "局所受容野 … 人間の視覚が認識する、限定された領域。  \n",
        "\n",
        "局所受容野の刺激に、脳で以下の２種類のニューロンが反応  \n",
        "単純型細胞 … ある特定の形状に反応。  \n",
        "複雑型細胞 … 形状の空間的なずれを吸収。  \n",
        "\n",
        "単純型細胞だけだと、ある形状の位置がずれると別の形状と見なすが、複雑型細胞は空間的な位置ずれを吸収し、同一形状と見なせるように働く。  \n",
        "\n",
        "CNNはこの2つの細胞の働きをマネする感じで作られている。  \n",
        "畳み込み層 … 単純型細胞に対応  \n",
        "プーリング層 … 複雑型細胞に対応  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ILYS0Ebo3Xn"
      },
      "source": [
        "\n",
        "### 畳み込みとは  \n",
        "ウィンドウとカーネルの重なった数値の積の合計を、ウィンドウを少しずつずらしながら全ての要素に対して計算し、最終的にテンソルに変換する処理のこと。  \n",
        "画像の縦，横，チャンネルの3次元データをそのまま学習可能。  \n",
        "\n",
        "カーネル／フィルター： 5×5のような格子状の数値データ。  \n",
        "ウィンドウ： カーネルと同サイズの部分画像。  \n",
        "特徴マップ： 畳み込みによって得られたテンソル。  \n",
        "ストライド： 畳み込み時にウィンドウを少しずつずらす幅。  \n",
        "パディング：  \n",
        "画像の周りに適当な数値の余白ピクセルを追加すること。これにより、画像の端の特徴を捉える。  \n",
        "プーリング：  \n",
        "ウィンドウの数値データから1つの数値を作り出す処理。画像の縮小が行える。  \n",
        "CNNの2つのパート：  \n",
        "畳み込みやプーリングにより特徴マップを作成する『特徴量抽出パート』と、  \n",
        "全結合層を繰り返すことで最終的な出力を得る『識別パート』\n",
        "\n",
        "https://www.atmarkit.co.jp/ait/articles/1804/23/news138.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTprGkitvX-I"
      },
      "source": [
        "## 2.5 最新のCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kjQNY5avc3g"
      },
      "source": [
        "### LeNet（古いCNN）  \n",
        "1998年に提唱された初のCNN  \n",
        "しかし以下の問題に直面し、computer visionの主流をSVMから奪えずにいた…  \n",
        "・活性化関数の勾配消失問題  \n",
        "・過学習の問題\n",
        "### AlexNet（新しいCNN）  \n",
        "コンペティションILSVRC（ImageNet Large Scale Visual Recognition Challenge）-2012  \n",
        "・ぶっちぎりの正答率  \n",
        "・もはや職人芸（SVMの様な）を必要としない  \n",
        "ここからcomputer visionといえばCNNに。いや、\n",
        "### Deep LearningはAlexNetから始まった！  \n",
        "Alexは論文著者の名前。（どうも共同著者のHinton氏がすごい人らしい。）  \n",
        "要は何が新しかったのか？  \n",
        "・ReLU（Rectified Linear Unit）の利用で勾配消失を回避  \n",
        "・dropOutの利用で過学習の回避  \n",
        "「なんだ…、当たり前のことやってんじゃん」という声が聞こえてくる。これらを当たり前にしたのがAlexNetなのだ。  \n",
        "\n",
        "## 御社への指摘\n",
        "「最新のCNN」がAlexNetを指しているなら、AlexNetはもはや最新ではないので適切ではない。  \n",
        "ニューラルネットの歴史を概観する際に、「古いCNN」LeNetと「新しいCNN」AlexNetを対比する視点は重要である。  \n",
        "速やかに「最新のCNN」を「新しいCNN」に直されることを期待する。\n",
        "\n"
      ]
    }
  ]
}