{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "2_3_2_dropout_hands_on.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanri3/deep_learning_day1_day2/blob/main/2_3_2_dropout_hands_on.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1666ec-5fa7-4aa6-d9a0-1fd63c454aaf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code')\n",
        "# sys.path.append('/content/drive/My Drive/DNN_code/lesson_2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCN6TX4-5F03",
        "outputId": "ee5296ca-1c0a-4609-d3d9-cee4ca015a7f"
      },
      "source": [
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common import layers, optimizer\n",
        "from data.mnist import load_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "# from common.multi_layer_net import MultiLayerNet\n",
        "# from multi_layer_net import MultiLayerNet\n",
        "# from common import optimizer\n",
        "import copy\n",
        "\n",
        "(x_train, tch_train), (x_test, tch_test) = load_mnist(normalize = True)\n",
        "print(\"x_train.shape :\", x_train.shape)\n",
        "print(\"データ読み込み完了\")\n",
        "\n",
        "# 過学習を再現するために、学習データを削減\n",
        "# x_train = x_train[:200]\n",
        "# tch_train = tch_train[:200]\n",
        "x_train = x_train[:300]\n",
        "tch_train = tch_train[:300]\n",
        "print(\"x_train :\", x_train)\n",
        "\n",
        "# network_init = MultiLayerNet(input_size=784, hidden_size_list = [100, 100, 100, 100, 100, 100], output_size = 10)\n",
        "\n",
        "optimizer_SDG_init = optimizer.SGD(learning_rate = 0.01)\n",
        "\n",
        "iters_num = 5\n",
        "# iters_num = 1000\n",
        "train_size = x_train.shape[0]\n",
        "# batch_size = 10\n",
        "batch_size = 100\n",
        "# plot_interval=50\n",
        "plot_interval=100\n",
        "\n",
        "# グラフ\n",
        "# def draw_graph(title, legend_position):\n",
        "#     lists = range(0, iters_num + 1, plot_interval)\n",
        "#     # lists = range(0, iters_num, plot_interval)\n",
        "#     # print(\"lists, accuracies_train :\", lists, accuracies_train)\n",
        "#     plt.plot(lists, accuracies_train, label=\"training set\")\n",
        "#     plt.plot(lists, accuracies_test,  label=\"test set\")\n",
        "#     plt.legend(loc = legend_position)\n",
        "#     # plt.legend(loc=\"lower right\")\n",
        "#     plt.title(title)\n",
        "#     plt.xlabel(\"count\")\n",
        "#     plt.ylabel(\"accuracy\")\n",
        "#     plt.ylim(0, 1.0)\n",
        "#     # グラフの表示\n",
        "#     plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape : (60000, 784)\n",
            "データ読み込み完了\n",
            "x_train : [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmdVHCf4pQ4h"
      },
      "source": [
        "## Multi Layer Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwGTs1ktpQ4i"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir) # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common import layers as lyrs\n",
        "\n",
        "class MultiLayerNet:\n",
        "    '''\n",
        "    input_size: 入力層のノード数\n",
        "    hidden_size_list: 隠れ層のノード数のリスト\n",
        "    output_size: 出力層のノード数\n",
        "    activation: 活性化関数\n",
        "    weight_init_std: 重みの初期化方法\n",
        "    weight_decay_lambda: L2正則化の強さ\n",
        "    use_dropout: ドロップアウトの有無\n",
        "    dropout_ratio: ドロップアウト率\n",
        "    use_batchnorm: バッチ正規化の有無\n",
        "    '''\n",
        "    def __init__(self, input_size, hidden_size_list, output_size, activation='relu', weight_init_std='relu', weight_decay_lambda=0,\n",
        "                 use_dropout = False, dropout_ratio = 0.5, use_batchnorm=False):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size_list = hidden_size_list\n",
        "        self.hidden_layer_num = len(hidden_size_list)\n",
        "        self.use_dropout = use_dropout\n",
        "        self.weight_decay_lambda = weight_decay_lambda\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.params = {}\n",
        "\n",
        "        # 重みの初期化\n",
        "        self.__init_weight(weight_init_std)\n",
        "\n",
        "        # レイヤの生成\n",
        "        activation_layer = {'sigmoid': lyrs.Sigmoid, 'relu': lyrs.Relu}\n",
        "        self.layers = OrderedDict()\n",
        "        print(\"self.hidden_layer_num :\", self.hidden_layer_num)\n",
        "        for idx in range(1, self.hidden_layer_num + 1):\n",
        "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "            # self.layers['Affine' + str(idx)] = lyrs.Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "            if self.use_batchnorm:\n",
        "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx - 1])\n",
        "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx - 1])\n",
        "                self.layers['BatchNorm' + str(idx)] = lyrs.BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
        "                \n",
        "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
        "            \n",
        "            if self.use_dropout:\n",
        "                self.layers['Dropout' + str(idx)] = Dropout(dropout_ratio)\n",
        "                # self.layers['Dropout' + str(idx)] = lyrs.Dropout(dropout_ratio)\n",
        "\n",
        "        idx = self.hidden_layer_num + 1\n",
        "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "        # self.layers['Affine' + str(idx)] = lyrs.Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "\n",
        "        self.last_layer = lyrs.SoftmaxWithLoss()\n",
        "        print(\"self.layers.items() :\", self.layers.items())\n",
        "        print(\"len self.layers :\", len(self.layers))\n",
        "\n",
        "    def __init_weight(self, weight_init_std):\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "        print(\"all_size_list\", all_size_list)\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            scale = weight_init_std\n",
        "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
        "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUを使う場合に推奨される初期値\n",
        "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
        "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidを使う場合に推奨される初期値\n",
        "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
        "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
        "            print(\"W\", idx, \" : \", self.params['W' + str(idx)].shape)\n",
        "            print(\"b\", idx, \" : \", self.params['b' + str(idx)].shape)\n",
        "\n",
        "    def predict(self, x_tr, train_flg = False):\n",
        "      print(\"x_tr.shape まるれいpredi :\", x_tr.shape)\n",
        "      for key, layer in self.layers.items():\n",
        "          if \"Dropout\" in key or \"BatchNorm\" in key:\n",
        "              print(\"x_tr.shape まるれいpredi ifない :\", x_tr.shape)\n",
        "              x_tr = layer.forward(x_tr, train_flg)\n",
        "          else:\n",
        "              x_tr = layer.forward(x_tr)\n",
        "      return x_tr\n",
        "\n",
        "    def loss(self, x_tr, tch_tr, train_flg = False):\n",
        "      print(\"x_tr.shape まるれいloss :\", x_tr.shape)\n",
        "      y = self.predict(x_tr, train_flg)\n",
        "      weight_decay = 0\n",
        "      for idx in range(1, self.hidden_layer_num + 2):\n",
        "          W = self.params['W' + str(idx)]\n",
        "          weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
        "      print(\"loss メソッドのreturn前\")\n",
        "      return self.last_layer.forward(y, tch_tr) + weight_decay\n",
        "\n",
        "    def accuracy(self, X, D):\n",
        "        Y = self.predict(X, train_flg=False)\n",
        "        Y = np.argmax(Y, axis=1)\n",
        "        if D.ndim != 1 : D = np.argmax(D, axis=1)\n",
        "        accuracy = np.sum(Y == D) / float(X.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    def gradient(self, x_tr, tch_tr):\n",
        "        # forward\n",
        "        print(\"x_tr.shape まるれいgrad :\", x_tr.shape)\n",
        "        self.loss(x_tr, tch_tr, train_flg = True)\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
        "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
        "\n",
        "            if self.use_batchnorm and idx != self.hidden_layer_num + 1:\n",
        "                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n",
        "                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
        "        return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFSJQRtS47sP"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Xbor5z5Y_S"
      },
      "source": [
        "class Dropout:\n",
        "    def __init__(self, dropout_ratio = 0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x_tr, train_flg = True):\n",
        "        print(\"x_tr.shape Dropout.forw():\", x_tr.shape)\n",
        "        print(\"train_flg :\", train_flg)\n",
        "        if train_flg:\n",
        "            # 引数の前のアスタリスクは可変長引数\n",
        "            # https://dev.classmethod.jp/articles/what-does-asterisk-mean-at-args/\n",
        "            self.mask = np.random.rand(*x_tr.shape) > self.dropout_ratio\n",
        "            # print(\"np.random.rand(*x_tr.shape) :\", np.random.rand(*x_tr.shape))\n",
        "            print(\"self.mask.shape :\", self.mask.shape)\n",
        "            returned_value = x_tr * self.mask\n",
        "            print(\"x_tr.shape :\", x_tr.shape)\n",
        "            print(\"(x_tr * self.mask).shape :\", returned_value.shape)\n",
        "        else:\n",
        "            returned_value = x_tr * (1.0 - self.dropout_ratio)\n",
        "            # print(\"x_tr * (1.0 - self.dropout_ratio) :\", x_tr * (1.0 - self.dropout_ratio))\n",
        "        return returned_value\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9hzPatWHSY8"
      },
      "source": [
        "## Affine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-76mJU_cHPpx"
      },
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W =W\n",
        "        self.b = b\n",
        "        print(\"aFFFFFFine\")\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 重み・バイアスパラメータの微分\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # テンソル対応        \n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYbEIg-fZw7O"
      },
      "source": [
        "## 実行部"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDyPfHnypQ4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cafe1c8-59ed-4233-fe1d-7c143fb9f78f"
      },
      "source": [
        "from common import optimizer\n",
        "\n",
        "# ドロップアウト設定 ======================================\n",
        "use_dropout = True\n",
        "# dropout_ratio = 0.3\n",
        "dropout_ratio = 0.08\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNet(input_size=784, \n",
        "                        hidden_size_list=[5, 5], \n",
        "                        # hidden_size_list=[100, 100, 100, 100, 100, 100], \n",
        "                        output_size=10,\n",
        "                        # weight_decay_lambda=weight_decay_lambda, \n",
        "                        use_dropout = use_dropout, dropout_ratio = dropout_ratio)\n",
        "optimizer = optimizer.SGD(learning_rate=0.01)\n",
        "# optimizer = optimizer.Momentum(learning_rate=0.01, momentum=0.9)\n",
        "# optimizer = optimizer.AdaGrad(learning_rate=0.01)\n",
        "# optimizer = optimizer.Adam()\n",
        "\n",
        "train_loss_list = []\n",
        "accuracies_train = []\n",
        "accuracies_test = []\n",
        "# plot_interval=10\n",
        "\n",
        "for i in range(iters_num):\n",
        "    print(\"iter loop_val i=\", i)\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    tch_batch = tch_train[batch_mask]\n",
        "    print(\"batch_mask :\", batch_mask)\n",
        "    print(\"x_train.shape :\", x_train.shape)\n",
        "    print(\"x_batch.shape :\", x_batch.shape)\n",
        "\n",
        "    print(\"x_batch.shape network.graaaa():\", x_batch.shape)\n",
        "    grad = network.gradient(x_batch, tch_batch)\n",
        "    optimizer.update(network.params, grad)\n",
        "\n",
        "    print(\"iter network.loss(\")\n",
        "    print(\"x_batch.shape network.loss():\", x_batch.shape)\n",
        "    loss = network.loss(x_batch, tch_batch)\n",
        "    train_loss_list.append(loss)    \n",
        "    \n",
        "    # if (i + 1) % plot_interval == 0 or i == 0:\n",
        "    #     accr_train = network.accuracy(x_train, tch_train)\n",
        "    #     accr_test = network.accuracy(x_test, tch_test)\n",
        "    #     accuracies_train.append(accr_train)\n",
        "    #     accuracies_test.append(accr_test)\n",
        "\n",
        "    #     print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(format(accr_train,'.5f')))\n",
        "    #     print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(format(accr_test,'.5f')))        \n",
        "\n",
        "# グラフの表示\n",
        "# draw_graph(\"drop out!!\", \"lower right\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_size_list [784, 5, 5, 10]\n",
            "W 1  :  (784, 5)\n",
            "b 1  :  (5,)\n",
            "W 2  :  (5, 5)\n",
            "b 2  :  (5,)\n",
            "W 3  :  (5, 10)\n",
            "b 3  :  (10,)\n",
            "self.hidden_layer_num : 2\n",
            "aFFFFFFine\n",
            "aFFFFFFine\n",
            "aFFFFFFine\n",
            "self.layers.items() : odict_items([('Affine1', <__main__.Affine object at 0x7efebd563450>), ('Activation_function1', <common.layers.Relu object at 0x7efebd563f50>), ('Dropout1', <__main__.Dropout object at 0x7efebd563b50>), ('Affine2', <__main__.Affine object at 0x7efebd563b10>), ('Activation_function2', <common.layers.Relu object at 0x7efebd563b90>), ('Dropout2', <__main__.Dropout object at 0x7efebd563d90>), ('Affine3', <__main__.Affine object at 0x7efebd563e90>)])\n",
            "len self.layers : 7\n",
            "iter loop_val i= 0\n",
            "batch_mask : [215  50 109 275  44 298  12 121  25  35 209  24  67  37 149 132  79 164\n",
            " 229 281 291  67 265 138  56 288 174  94  76 164 243  10 268  41 235 235\n",
            " 183 150  23 127  33 225 267  64  81 192 154 195  84 213 271 110 272  92\n",
            " 255 225 153 125 157 233 110 104 158 142 257 185 106  43 183 103 210 257\n",
            " 240 275 257  75   4 235 253 131 144 154 289  39 180 114 288 262 168   5\n",
            "  28 288  47 266  47 131 114  69 196  52]\n",
            "x_train.shape : (300, 784)\n",
            "x_batch.shape : (100, 784)\n",
            "x_batch.shape network.graaaa(): (100, 784)\n",
            "x_tr.shape まるれいgrad : (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "loss メソッドのreturn前\n",
            "iter network.loss(\n",
            "x_batch.shape network.loss(): (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "loss メソッドのreturn前\n",
            "iter loop_val i= 1\n",
            "batch_mask : [283 167  65 191 170  26 276 125   5  79  80 236 293 267 108 252 115  46\n",
            " 118  99  98 257 158 214 244  71  12 291 166 182 193  50 158  71  30 266\n",
            " 252  55 169 164 256  81  82 215 199 243 109 242  24 183 216 297  74  14\n",
            " 137 245 272 237  24 295 242  37  17  14  67 223  91   0 121 120 134 157\n",
            " 210 135 186 189 129 158  58  54 289  72 234  38 242  71  84 282   5 163\n",
            "  35 269 104 209  40 170 170 223 167 178]\n",
            "x_train.shape : (300, 784)\n",
            "x_batch.shape : (100, 784)\n",
            "x_batch.shape network.graaaa(): (100, 784)\n",
            "x_tr.shape まるれいgrad : (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "loss メソッドのreturn前\n",
            "iter network.loss(\n",
            "x_batch.shape network.loss(): (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "loss メソッドのreturn前\n",
            "iter loop_val i= 2\n",
            "batch_mask : [294 103  67 247 119 119 244 210 278  30   4  82  59 170  94 163 249  79\n",
            "   8 214  36  15  28 130 138 256 120  49 124  90  71 136   1  13  58  89\n",
            " 167 257  67 162 120 295 163   0 247 255 152 154  86 119 140  97 272  35\n",
            " 257 181 194   5 181 135 106 101 196  64  13  64 241 210 103 126 255 246\n",
            " 245 223 268  80 235  79 243 176  68 205 298  79 271 122  22  99 228  97\n",
            " 118 240 177 134 243 284 297 140  76 165]\n",
            "x_train.shape : (300, 784)\n",
            "x_batch.shape : (100, 784)\n",
            "x_batch.shape network.graaaa(): (100, 784)\n",
            "x_tr.shape まるれいgrad : (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "loss メソッドのreturn前\n",
            "iter network.loss(\n",
            "x_batch.shape network.loss(): (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "loss メソッドのreturn前\n",
            "iter loop_val i= 3\n",
            "batch_mask : [128 221 217  75  78  89 222   0 222  95  44  44 227 103 244  85 102  66\n",
            "   2  53 202  19 163  76 122 225 113 115 114  47 255 203 126 264 161 238\n",
            " 141  11   1  37 115 215 290 157 131 166 270  54 175 222  17  59  63 158\n",
            " 271 281 100 101  91   7  34 177  83  18 212 243 216  29 199   8 149 113\n",
            " 173 208 160 186 197 112 180  84 154 270 149 297  11 190 215 254   7  28\n",
            " 174 239 225 223 263  62 122  44 292 175]\n",
            "x_train.shape : (300, 784)\n",
            "x_batch.shape : (100, 784)\n",
            "x_batch.shape network.graaaa(): (100, 784)\n",
            "x_tr.shape まるれいgrad : (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "loss メソッドのreturn前\n",
            "iter network.loss(\n",
            "x_batch.shape network.loss(): (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "loss メソッドのreturn前\n",
            "iter loop_val i= 4\n",
            "batch_mask : [  4  87 291 287 291  23 176 257  15 278 117 185 280 145 299  46  11 181\n",
            " 278 201  42 102  84 293 118 209 162  46 262  13  58 187 217 283 268 207\n",
            "  79  91 100 170  42  51  58 282  20 264 143  52 164  95 182 105 134  19\n",
            "  60 165 192 213 123 174 228  45 251 176  38 148 186 297  96 239 143  18\n",
            " 184 146 271  97 259  52 150  11 170  84   9 158 148  95 185  28 121 169\n",
            " 281 112  98  52 174  66 236  49 284  58]\n",
            "x_train.shape : (300, 784)\n",
            "x_batch.shape : (100, 784)\n",
            "x_batch.shape network.graaaa(): (100, 784)\n",
            "x_tr.shape まるれいgrad : (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : True\n",
            "self.mask.shape : (100, 5)\n",
            "x_tr.shape : (100, 5)\n",
            "(x_tr * self.mask).shape : (100, 5)\n",
            "loss メソッドのreturn前\n",
            "iter network.loss(\n",
            "x_batch.shape network.loss(): (100, 784)\n",
            "x_tr.shape まるれいloss : (100, 784)\n",
            "x_tr.shape まるれいpredi : (100, 784)\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "x_tr.shape まるれいpredi ifない : (100, 5)\n",
            "x_tr.shape Dropout.forw(): (100, 5)\n",
            "train_flg : False\n",
            "loss メソッドのreturn前\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A5klEiDHq30",
        "outputId": "0bcce963-2c2f-41c3-b60f-cca27c827d35"
      },
      "source": [
        "mat_01 = np.array([[1, 2], [3, 4]])\n",
        "mat_bool = np.array([[True, False], [True, False]])\n",
        "mat_01 * mat_bool"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0],\n",
              "       [3, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}